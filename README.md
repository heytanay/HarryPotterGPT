# GPT Pre-training from Scratch

![](gpt_arch.png)

This repository is my attempt on writing a GPT model and pre-training it from scratch on data openly found on the internet, in this case â€“ the [Bookcorpus dataset](https://www.kaggle.com/datasets/krishbaisoya/english-bookcorpus) on Kaggle.

I have currently only trained the model on a very small subset of this data because of resource constraints and because of my decision to see how good of a model one can train for free with only his/her skills.

I wrote the GPT model from scratch with a lot of inspiration and help form Andrej Karpathy's [nanoGPT](https://github.com/karpathy/nanoGPT). Huge shoutout and thanks to his work in educating millions about these LLMs.

In this README, you will find several notes on model architecture, data tokenization, data loading and in-general model training.

I am currently in the process of writing a blog post where I will have detailed several approaches on how to train the model and load data efficiently as well as my observations while doing this.

